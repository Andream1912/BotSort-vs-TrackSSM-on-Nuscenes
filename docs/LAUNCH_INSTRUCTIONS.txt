#!/bin/bash

# ISTRUZIONI PER DOMANI QUANDO GPU DISPONIBILE

cat << 'EOF'
================================================================================
ðŸš€ GUIDA RAPIDA - Training Phase 1 TrackSSM su NuScenes
================================================================================

ðŸ“‹ STATO CORRENTE:
  âœ… Dataset generato: 4,200 train sequences (4.5M track samples)
  âœ… Dataset loader: NuScenesTrackDataset (formato track-level compatibile)
  âœ… Encoder corretto: Time_info_aggregation (non Mamba_encoder diretto)
  âœ… Training script: train_phase1_decoder.py (testato sintatticamente)
  âœ… Config GPU: nuscenes_phase1_gpu.yaml (batch=64, workers=12)

âš ï¸  PROBLEMA CORRENTE:
  âŒ GPU non disponibile â†’ training non puÃ² partire
  â¸ï¸  In attesa che GPU torni disponibile

================================================================================
ðŸ“ COSA FARE DOMANI MATTINA
================================================================================

OPZIONE 1: Verifica e Lancia Manualmente
-----------------------------------------

1. Verifica GPU disponibile:
   
   python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

2. Se GPU disponibile, esegui pre-flight check:
   
   bash scripts/utils/preflight_check.sh

3. Se tutto OK, lancia training:
   
   cd /user/amarino/tesi_project_amarino/trackssm_reference
   
   nohup python scripts/training/train_phase1_decoder.py \
     --config configs/nuscenes_phase1_gpu.yaml \
     --data_root ./data/nuscenes_mot_6cams_interpolated \
     --output_dir weights/phase1 \
     > logs/phase1_training_gpu.log 2>&1 &
   
   # Salva PID
   echo $! > /tmp/training_pid.txt


OPZIONE 2: Lancia GPU Watcher (CONSIGLIATO)
--------------------------------------------

Lancia script che attende GPU e parte automaticamente:

  nohup bash scripts/utils/gpu_watcher.sh > logs/gpu_watcher.log 2>&1 &

Questo script:
  - Controlla ogni secondo se GPU Ã¨ disponibile
  - Appena disponibile, lancia automaticamente il training
  - Non devi fare nulla, funziona in background

================================================================================
ðŸ“Š MONITORAGGIO TRAINING
================================================================================

Una volta partito il training:

# Dashboard veloce
bash scripts/utils/monitor_training.sh

# Segui log in tempo reale
tail -f logs/phase1_training_gpu.log

# Auto-refresh ogni minuto
watch -n 60 bash scripts/utils/monitor_training.sh

# Report completo
bash scripts/utils/night_report.sh

================================================================================
âš™ï¸  PARAMETRI TRAINING
================================================================================

Config: nuscenes_phase1_gpu.yaml
  - Encoder: Time_info_aggregation (d_model=256, n_layer=2)
  - Decoder: D2MP (6 decoder layers)
  - Batch size: 64
  - Workers: 12
  - Learning rate: 1e-4
  - Epochs: 40
  - Device: CUDA
  - Early stopping: patience=7

Dataset: NuScenesTrackDataset
  - Train samples: 4,535,856 track windows
  - Val samples: ~1,000,000 track windows
  - Format: (B, 5, 8) - 5 history frames, 8 features [bbox+delta]
  - Tracks: 87,294 individual tracks extracted

Training Strategy:
  - Phase 1: FREEZE encoder, train decoder only (~50% params)
  - Optimizer: AdamW (lr=1e-4, weight_decay=1e-4)
  - Scheduler: CosineAnnealingLR (warmup=3 epochs)
  - Checkpoint: Salvato ogni 5 epoche + best model

================================================================================
â±ï¸  TEMPI STIMATI
================================================================================

GPU (batch=64, 12 workers):
  - Per epoch: ~40-50 minuti
  - 40 epochs: ~12-16 ore
  - Training completo: ~12-16 ore

================================================================================
ðŸ”§ TROUBLESHOOTING
================================================================================

Se training non parte:

1. Verifica GPU:
   nvidia-smi
   python -c "import torch; print(torch.cuda.is_available())"

2. Verifica dataset:
   ls -la data/nuscenes_mot_6cams_interpolated/train | wc -l
   # Dovrebbe essere ~4200

3. Test manuale (1 batch):
   timeout 120 python scripts/training/train_phase1_decoder.py \
     --config configs/nuscenes_phase1_gpu.yaml \
     --data_root ./data/nuscenes_mot_6cams_interpolated \
     --output_dir weights/phase1_test

4. Controlla log errori:
   tail -100 logs/phase1_training_gpu.log

================================================================================
ðŸ“‚ FILE IMPORTANTI
================================================================================

Scripts:
  scripts/training/train_phase1_decoder.py  â†’ Main training script
  scripts/utils/preflight_check.sh          â†’ Verifica pre-lancio
  scripts/utils/gpu_watcher.sh              â†’ Auto-launch quando GPU ready
  scripts/utils/monitor_training.sh         â†’ Dashboard monitoraggio
  scripts/utils/night_report.sh             â†’ Report completo risultati

Configs:
  configs/nuscenes_phase1_gpu.yaml          â†’ Config ottimizzata GPU
  configs/nuscenes_phase1.yaml              â†’ Config CPU (fallback)

Dataset:
  dataset/nuscenes_track_dataset.py         â†’ Dataset loader track-level
  data/nuscenes_mot_6cams_interpolated/     â†’ Dataset generato

Logs:
  logs/phase1_training_gpu.log              â†’ Log training principale
  logs/gpu_watcher.log                      â†’ Log GPU watcher

Weights:
  weights/phase1/phase1_decoder_best.pth    â†’ Best model (val loss)
  weights/phase1/phase1_decoder_epoch*.pth  â†’ Checkpoint periodici

================================================================================
âœ… CHECKLIST PRE-LANCIO
================================================================================

Prima di lanciare training, verifica:

  â–¡ GPU disponibile (torch.cuda.is_available() = True)
  â–¡ Dataset presente (4,200 train sequences)
  â–¡ Script sintatticamente corretto
  â–¡ Config GPU caricabile
  â–¡ Spazio disco sufficiente (>50GB per checkpoints)
  â–¡ Nessun altro training in esecuzione

Se tutto OK:
  bash scripts/utils/preflight_check.sh  # Verifica automatica
  bash scripts/utils/gpu_watcher.sh      # Lancia con auto-start

================================================================================
ðŸŽ¯ OBIETTIVI PHASE 1
================================================================================

Al termine del training, dovresti avere:

  âœ… Decoder fine-tuned su NuScenes multi-camera
  âœ… Best model checkpoint (~val loss < 0.5)
  âœ… Training log completo (tensorboard + text)
  âœ… Pronto per Phase 2: full fine-tuning

Metriche attese (dopo Phase 1):
  - Val loss: ~0.3-0.5 (decoder ancora imperfetto)
  - Train loss: ~0.2-0.4

Phase 2 migliorerÃ  ulteriormente fine-tuning anche l'encoder.

================================================================================

Per domande o problemi, controlla:
  docs/RESOLUTION_PLAN.md  â†’ Analisi completa problema architetturale
  README.md                â†’ Documentazione generale progetto

================================================================================
EOF
