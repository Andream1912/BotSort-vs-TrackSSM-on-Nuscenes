# Confronto Configurazioni Training YOLOX-L

## ğŸ¯ Obiettivo: Training Stabile per Presentazione Tesi

---

## ğŸ“Š TRAINING 1: Original (10 epochs) - BASELINE

### Configurazione
```python
max_epoch = 10
batch_size = 8
basic_lr = 0.001 / 64.0
LR effettivo = 0.000125
warmup_epochs = 1
no_aug_epochs = 2
mosaic_prob = 0.5
mixup_prob = 0.5
scheduler = "yoloxwarmcos"
```

### Risultati
- **Loss**: 9.92 â†’ 4.44 (-55.2%)
- **Max conf**: 0.868
- **Training time**: ~4 ore
- **Tracking (con match=0.85)**: IDSW **2646**, MOTA 33.85%
- **Convergenza**: Rapida ma oscillatoria

### Valutazione
âœ… **PRO**: 
- Risultati tracking eccellenti
- Training veloce
- Batte BotSort (-3.9% IDSW)

âŒ **CONTRO**:
- Curve oscillatorie
- Poche epoche per convergenza smooth

---

## ğŸ“Š TRAINING 2: Smooth (30 epochs) - ESTESO

### Configurazione
```python
max_epoch = 30
batch_size = 32          # +400% vs baseline
basic_lr = 0.001 / 64.0
LR effettivo = 0.0005    # +400% vs baseline (batch 4Ã—)
warmup_epochs = 1
no_aug_epochs = 2        # Solo ultime 2 epoche
mosaic_prob = 0.5        # Alta varianza
mixup_prob = 0.5         # Alta varianza
min_lr_ratio = 0.01
```

### Risultati
- **Loss**: 6.99 â†’ 5.01 (-28.3%)
- **Training time**: ~13 ore
- **Tracking**: IDSW **3076** (+219 vs epoch 10), MOTA 36.74%
- **Convergenza**: âŒ **MOLTO OSCILLATORIA**

### Valutazione
âœ… **PRO**:
- MOTA/IDF1/HOTA migliorati
- PiÃ¹ epoche per curva smooth

âŒ **CONTRO**:
- âŒâŒâŒ **Oscillazioni AMPIE** (problema principale!)
- IDSW peggiore (+219 switches)
- LR troppo alto (0.0005 per 28 epoche)
- Aug forte fino epoch 28
- Non presentabile per tesi

---

## ğŸ“Š TRAINING 3: Stable V3 (30 epochs) - OTTIMIZZATO âœ¨

### Configurazione Ultra Stabile
```python
max_epoch = 30
batch_size = 32          # Large batch per gradient stability
basic_lr = 0.0006 / 64.0 # -40% vs Training 2 ğŸ”¥
LR effettivo = 0.0003    # -40% reduction per smooth convergence
warmup_epochs = 3        # +200% vs Training 2 ğŸ”¥
no_aug_epochs = 8        # +300% vs Training 2 ğŸ”¥
mosaic_prob = 0.2        # -60% vs Training 2 ğŸ”¥
mixup_prob = 0.15        # -70% vs Training 2 ğŸ”¥
min_lr_ratio = 0.10      # +900% vs Training 2 (decay graduale)
degrees = 3.0            # -40% (rotation conservativa)
translate = 0.08         # -20% (traslazione conservativa)
shear = 0.5              # -50% (shear conservativo)
mosaic_scale = (0.85, 1.15)  # Range piÃ¹ stretto
```

### Schedule Ottimizzato
```
Epoch 1-3:   ğŸŒ¡ï¸  Warmup       0 â†’ 0.0003 (graduale, 3 epoche)
Epoch 4-22:  ğŸƒ Training      0.0003 (plateau con aug MINIMAL)
Epoch 23-30: ğŸ¯ Fine-tune    0.0003 â†’ 0.00003 (no-aug, decay smooth)
```

### Confronto Parametri Chiave

| Parametro | Training 2 (Smooth) | Training 3 (Stable) | Î” |
|-----------|---------------------|---------------------|---|
| **LR max** | 0.00050 | **0.00030** | **-40%** ğŸ”¥ |
| **LR min** | 0.000005 | **0.00003** | **+500%** ğŸ”¥ |
| **Warmup** | 1 epoch | **3 epochs** | **+200%** ğŸ”¥ |
| **No-aug** | 2 epochs | **8 epochs** | **+300%** ğŸ”¥ |
| **Mosaic prob** | 0.5 | **0.2** | **-60%** ğŸ”¥ |
| **Mixup prob** | 0.5 | **0.15** | **-70%** ğŸ”¥ |
| **Aug rotation** | Â±5Â° | **Â±3Â°** | **-40%** |
| **Aug translate** | 0.1 | **0.08** | **-20%** |
| **Aug shear** | 1.0 | **0.5** | **-50%** |
| **Scale range** | 0.8-1.2 | **0.85-1.15** | **-37.5%** |

### Impatto Atteso

#### ğŸ¯ Riduzione Oscillazioni
- **Varianza batch**: -60% (mosaic/mixup drasticamente ridotti)
- **LR instability**: -40% (learning rate piÃ¹ conservativo)
- **Aug variance**: -50% (parametri geometrici ridotti)
- **Fine-tune phase**: 8 epoche vs 2 (4Ã— piÃ¹ lungo)

#### ğŸ“ˆ Convergenza Attesa
```
Loss variance reduction: ~70-80%
Training curve smoothness: Alta (presentabile per tesi)
Final performance: Simile a Training 2 ma piÃ¹ stabile
IDSW target: < 2900 (meglio di Training 2)
MOTA target: ~35-36% (simile a Training 2)
```

### Vantaggi Chiave
âœ… **Warmup esteso (3 ep)**: Stabilizzazione iniziale robusta  
âœ… **LR ridotto 40%**: Convergenza graduale, meno overshoot  
âœ… **Aug minima**: Solo 20% mosaic, 15% mixup â†’ batch uniformi  
âœ… **Fine-tune lungo (8 ep)**: 1/4 del training su immagini pulite  
âœ… **Decay graduale**: LR finale 0.00003 vs 0.000005 (6Ã— piÃ¹ alto)  
âœ… **Parametri conservativi**: Rotation, translate, shear ridotti  
âœ… **Scale range stretto**: Meno distorsione oggetti  

### Trade-off
âš–ï¸ **Performance vs Stability**:
- Possibile leggera riduzione accuratezza (-0.5-1% MOTA)
- Ma stabilitÃ  curva training +80%
- **Accettabile per presentazione tesi** âœ¨

---

## ğŸ¬ PIANO DI LANCIO

### 1. Preparazione
```bash
cd /user/amarino/tesi_project_amarino
```

### 2. Verifica Configurazione
```bash
cd yolox_finetuning
cat configs/yolox_l_nuscenes_stable.py | grep -E "max_epoch|batch_size|basic_lr|warmup|no_aug|mosaic|mixup"
```

### 3. Lancio Training
```bash
cd /user/amarino/tesi_project_amarino
nohup python yolox_finetuning/scripts/train_stable.py > yolox_finetuning/training_stable.log 2>&1 &
```

### 4. Monitoraggio
```bash
# Segui il training in tempo reale
tail -f yolox_finetuning/training_stable.log

# Verifica ultimo checkpoint
ls -lth yolox_finetuning/yolox_l_nuscenes_stable/*.pth | head -5

# Controlla loss ogni 10 minuti
watch -n 600 'grep "iter.*loss" yolox_finetuning/training_stable.log | tail -20'
```

### 5. Stima Tempi
```
Tempo per epoca: ~26 minuti
Totale 30 epoche: ~13 ore
Checkpoint salvati: epochs 1, 5, 10, 15, 20, 25, 30
```

---

## ğŸ“Š VALUTAZIONE POST-TRAINING

### Plot Training Curves
```bash
cd yolox_finetuning
# Aggiorna plot_training.py per leggere training_stable.log
python plot_training.py
```

### Evaluation Tracking
```bash
# Test epoch 10
python track.py --tracker trackssm \
    --detector-weights yolox_finetuning/yolox_l_nuscenes_stable/epoch_10.pth \
    --data data/nuscenes_mot_front/val \
    --output results/TRACKSSM_STABLE_EPOCH10 \
    --conf-thresh 0.3 --match-thresh 0.85 --evaluate

# Test epoch 30
python track.py --tracker trackssm \
    --detector-weights yolox_finetuning/yolox_l_nuscenes_stable/epoch_30.pth \
    --data data/nuscenes_mot_front/val \
    --output results/TRACKSSM_STABLE_EPOCH30 \
    --conf-thresh 0.3 --match-thresh 0.85 --evaluate
```

### Confronto Finale
```bash
# Confronta i 3 training
python scripts/compare_trainings.py \
    --training1 results/TRACKSSM_FINETUNED_EPOCH10 \
    --training2 results/TRACKSSM_FINETUNED_30EPOCH \
    --training3 results/TRACKSSM_STABLE_EPOCH30 \
    --output presentation/training_comparison.png
```

---

## ğŸ¯ SUCCESSO ATTESO

### Curve Training
- âœ… Loss decresce monotonicamente (Â± 5% varianza)
- âœ… Smooth convergence visibile
- âœ… No spike > 10% loss increase
- âœ… Presentabile in slide tesi

### Metriche Tracking
- ğŸ¯ IDSW target: 2700-2900 (migliore di Training 2)
- ğŸ¯ MOTA target: 34-36% (simile a Training 2)
- ğŸ¯ Precision: > 87% (alta, come Training 1)

### Per Tesi
- âœ… Mostra 3 training curves comparati
- âœ… Spiega evoluzione: fast â†’ extended â†’ stable
- âœ… Evidenzia trade-off: speed vs stability vs performance
- âœ… Conclusione: Training 3 ottimale per production

---

## ğŸ“ NOTE TECNICHE

### Freeze Strategy (identica a tutti i training)
```
â„ï¸  FROZEN (86.1%):
   - CSPDarknet53 backbone (27.1M params)
   - PAFPN neck (19.5M params)

ğŸ”¥ TRAINABLE (13.9%):
   - YOLOXHead detection head (7.6M params)
```

### Dimensione Effettiva LR
```python
# Training 1 (baseline)
LR = 0.001/64 Ã— 8 = 0.000125

# Training 2 (smooth)
LR = 0.001/64 Ã— 32 = 0.000500  # 4Ã— piÃ¹ alto!

# Training 3 (stable) ğŸ”¥
LR = 0.0006/64 Ã— 32 = 0.000300  # Bilanciato, -40%
```

### Gradient Accumulation (considerato ma non necessario)
```python
# Con batch 32 giÃ  stabile, accumulation non serve
# Se necessario futuro:
accumulation_steps = 2  # Effective batch = 64
LR = 0.0006/64 Ã— 64 = 0.0006  # Scale accordingly
```

---

**Data creazione**: 5 dicembre 2025  
**Versione**: Training Config V3 - Ultra Stable  
**Status**: âœ… Pronto per lancio  
**Output atteso**: Curve smooth presentabili per tesi
