# Analisi Oscillazioni Training YOLOX-L (30 Epoch)

## üìä STATO ATTUALE

### Configurazione Training Completato
```
Training: 30 epoche (completato il 5 dic 2025)
Batch size: 32
Dataset: 21,332 immagini NuScenes
Iterazioni/epoch: 666
Loss reduction: 6.99 ‚Üí 5.01 (-28.3%)
```

### Problema Identificato
**Le loss curves mostrano oscillazioni significative durante tutto il training**, non convergono in modo smooth nonostante batch size 32.

---

## üîç ANALISI DETTAGLIATA

### 1. Architettura YOLOX-L

```
Input: 800x1440 (aspect ratio NuScenes)
       ‚Üì
CSPDarknet53 (backbone)      27,075,968 params (50.0%) ‚ùÑÔ∏è  FROZEN
       ‚Üì
PAFPN (neck)                 19,523,072 params (36.0%) ‚ùÑÔ∏è  FROZEN
       ‚Üì
YOLOXHead (detection)         7,553,572 params (14.0%) üî• TRAINABLE
       ‚Üì
Output: 7 classi NuScenes
```

**Totale**: 54,152,612 parametri
- **Frozen**: 46,599,040 (86.1%) - Feature extraction
- **Trainable**: 7,553,572 (13.9%) - Adattamento task-specific

### 2. Distribuzione Parametri per Layer

| Layer | Parametri | % | Status |
|-------|-----------|---|--------|
| CSPDarknet.stem | 7,040 | 0.01% | ‚ùÑÔ∏è  FROZEN |
| CSPDarknet.dark2 | 230,912 | 0.43% | ‚ùÑÔ∏è  FROZEN |
| CSPDarknet.dark3 | 1,906,688 | 3.52% | ‚ùÑÔ∏è  FROZEN |
| CSPDarknet.dark4 | 7,614,464 | 14.06% | ‚ùÑÔ∏è  FROZEN |
| **CSPDarknet.dark5** | **17,316,864** | **31.98%** | ‚ùÑÔ∏è  FROZEN |
| PAFPN.lateral_conv0 | 525,312 | 0.97% | ‚ùÑÔ∏è  FROZEN |
| PAFPN.C3_p4 | 2,757,632 | 5.09% | ‚ùÑÔ∏è  FROZEN |
| PAFPN.reduce_conv1 | 131,584 | 0.24% | ‚ùÑÔ∏è  FROZEN |
| PAFPN.C3_p3 | 690,688 | 1.28% | ‚ùÑÔ∏è  FROZEN |
| PAFPN.bu_conv2 | 590,336 | 1.09% | ‚ùÑÔ∏è  FROZEN |
| PAFPN.C3_n3 | 2,495,488 | 4.61% | ‚ùÑÔ∏è  FROZEN |
| PAFPN.bu_conv1 | 2,360,320 | 4.36% | ‚ùÑÔ∏è  FROZEN |
| **PAFPN.C3_n4** | **9,971,712** | **18.41%** | ‚ùÑÔ∏è  FROZEN |
| **YOLOXHead** | **7,553,572** | **13.95%** | üî• **TRAINABLE** |

### 3. Configurazione Training (Completato)

```python
# Training settings
max_epoch = 30
warmup_epochs = 1          # ‚ö†Ô∏è  Troppo corto!
no_aug_epochs = 2          # ‚ö†Ô∏è  Aug attiva fino epoch 28!

# Learning rate
batch_size = 32            # ‚úÖ OK
basic_lr_per_img = 0.001/64 = 0.00001562
LR effettivo = 0.00001562 √ó 32 = 0.000500  # ‚ö†Ô∏è  Troppo alto!
min_lr_ratio = 0.01        # ‚ö†Ô∏è  Decay troppo aggressivo!
scheduler = "yoloxwarmcos" # ‚úÖ OK (cosine annealing)

# Data augmentation
mosaic_prob = 0.5          # ‚ö†Ô∏è  Alta varianza batch!
mixup_prob = 0.5           # ‚ö†Ô∏è  Alta varianza batch!
hsv_prob = 0.5
flip_prob = 0.5
```

### 4. Learning Rate Schedule

```
Warmup (epoch 1): 0 ‚Üí 0.0005 (lineare)
Constant (epoch 2-28): 0.0005 (plateau)
No-aug (epoch 29-30): 0.0005 ‚Üí 0.000005 (cosine decay)

Totale iterazioni: 666 √ó 30 = 19,980
```

---

## ‚ö†Ô∏è  CAUSE DELLE OSCILLAZIONI

### 1. üé≤ **Varianza nei Mini-Batch** (CAUSA PRINCIPALE)
- Dataset: 21,332 immagini
- Mosaic prob 0.5: 50% batch usa mosaic (4 immagini fuse)
- Mixup prob 0.5: 50% batch usa mixup (2 immagini blend)
- **Risultato**: Alcuni batch molto difficili (oggetti small, overlap), altri facili
- Loss oscilla in base alla "fortuna" del batch random

### 2. üìà **Learning Rate Troppo Alto**
- LR = 0.0005 per 28 epoche consecutive
- Nessun decay graduale fino a epoch 29
- Con solo 7.5M params trainable, LR 0.0005 √® eccessivo
- **Confronto**: Training 10-epoch originale usava batch 8, quindi LR effettivo = 0.000125 (4x pi√π basso!)

### 3. üìö **Dataset Piccolo**
- Solo 21K immagini, 666 iter/epoch
- Alta varianza statistica batch-to-batch
- Ogni epoch vede tutte le immagini ma con augmentation diversa
- Impossibile stabilizzare completamente

### 4. ‚è±Ô∏è  **Augmentation Troppo Lunga**
- Strong aug (mosaic/mixup) fino a epoch 28
- Solo 2 epoche finali senza aug per stabilizzazione
- Il modello non ha tempo di "fine-tune" su immagini pulite

---

## ‚úÖ SOLUZIONI IMPLEMENTATE (Config V2)

### Modifiche Applicate:

```python
# Training settings - SMOOTH CONVERGENCE V2
warmup_epochs = 2          # Era 1 ‚Üí +100% stabilizzazione iniziale
no_aug_epochs = 5          # Era 2 ‚Üí Ultime 5 epoche senza aug (25-30)

# Learning rate - OPTIMIZED
basic_lr_per_img = 0.0008/64  # Era 0.001/64 ‚Üí -20% LR
# LR effettivo: 0.0008/64 √ó 32 = 0.000400 (era 0.000500)
min_lr_ratio = 0.05        # Era 0.01 ‚Üí Decay meno aggressivo

# Augmentation - REDUCED
mosaic_prob = 0.3          # Era 0.5 ‚Üí -40% varianza
mixup_prob = 0.3           # Era 0.5 ‚Üí -40% varianza
```

### Nuovo Schedule:

```
Epoch 1-2:   Warmup 0 ‚Üí 0.0004 (stabilizzazione)
Epoch 3-25:  Constant 0.0004 (training principale con aug ridotta)
Epoch 26-30: No aug + decay 0.0004 ‚Üí 0.00002 (fine-tuning smooth)
```

### Impatto Atteso:

| Metrica | Prima | Dopo | Delta |
|---------|-------|------|-------|
| LR max | 0.0005 | 0.0004 | -20% |
| LR min | 0.000005 | 0.00002 | +300% |
| Warmup | 1 epoch | 2 epochs | +100% |
| No-aug | 2 epochs | 5 epochs | +150% |
| Mosaic/Mixup | 0.5 | 0.3 | -40% |
| Loss variance | Alta | **Media** | ‚¨áÔ∏è  |

---

## üí° ALTERNATIVA AVANZATA: Unfreeze Graduale

### Strategia Progressive Unfreezing:

```python
# Epoch 1-10: Solo YOLOXHead trainable
frozen = ['backbone.backbone.*', 'backbone.lateral_conv0', 
          'backbone.C3_p*', 'backbone.reduce_conv1', 
          'backbone.bu_conv*', 'backbone.C3_n*']
trainable = ['head.*']
params = 7.5M (14%)

# Epoch 11-20: Unfreeze PAFPN neck
frozen = ['backbone.backbone.*']  # Solo CSPDarknet
trainable = ['backbone.lateral_conv0', 'backbone.C3_p*', 
             'backbone.reduce_conv1', 'backbone.bu_conv*', 
             'backbone.C3_n*', 'head.*']
params = 27M (50%)

# Epoch 21-30: Unfreeze dark5 (high-level features)
frozen = ['backbone.backbone.stem', 'backbone.backbone.dark[2-4]']
trainable = ['backbone.backbone.dark5.*', 'backbone.lateral_conv0', 
             'backbone.C3_p*', 'backbone.reduce_conv1', 
             'backbone.bu_conv*', 'backbone.C3_n*', 'head.*']
params = 44M (81%)
```

### Vantaggi:
- ‚úÖ Convergenza pi√π stabile (meno params all'inizio)
- ‚úÖ Feature adaptation graduale (head ‚Üí neck ‚Üí backbone)
- ‚úÖ Previene catastrophic forgetting (COCO features)
- ‚úÖ Permette LR pi√π alto inizialmente

### Svantaggi:
- ‚ùå Pi√π complesso da implementare
- ‚ùå Richiede tuning LR per ogni fase
- ‚ùå Tempo totale training identico (30 epoche)

---

## üìà PROSSIMI PASSI

### Opzione 1: Re-train con Config V2 (RACCOMANDATO)
```bash
cd yolox_finetuning
nohup python scripts/train_smooth.py > training_smooth_v2.log 2>&1 &
```
**Stima tempo**: ~13 ore
**Output**: `yolox_l_nuscenes_smooth_v2/epoch_*.pth`

### Opzione 2: Usa Epoch 10 Training Originale (GI√Ä OTTIMALE)
- Training 10-epoch aveva batch 8, LR 0.000125 (pi√π basso)
- Risultati: IDSW 2857, MOTA 33.28%
- Gi√† batte BotSort con match_thresh=0.85 ‚Üí IDSW 2646
- **Conclusione**: Epoch 10 gi√† ottimo per production

### Opzione 3: Test Epoch 30 con conf_thresh=0.4
- Training 30-epoch: IDSW 3076 ma MOTA 36.74%
- Problema: troppi false positives (FP 3,189 vs 2,040)
- **Soluzione**: Aumenta conf_thresh 0.3 ‚Üí 0.4 per filtrare FP
- Stima tempo: ~25 minuti evaluation

---

## üéØ RACCOMANDAZIONE FINALE

**Non rifare il training di 30 epoche.**

### Motivazioni:
1. ‚úÖ **Epoch 10 originale gi√† ottimale**: IDSW 2646 batte BotSort 2754
2. ‚è±Ô∏è  **Tempo vs beneficio**: 13 ore training per oscillazioni leggermente ridotte
3. üìä **Trade-off noto**: Epoch 30 migliora MOTA ma peggiora IDSW
4. üî¨ **Per tesi**: Mostra training curves epoch 10 (convergenza veloce) vs epoch 30 (overfit detection)

### Strategia Ottimale:
1. Usa **epoch 10 come production model**
2. Analizza trade-off tra epoch 10 e 30 nella tesi
3. Documenta oscillazioni come "limite dataset size"
4. Evidenzia successo: TrackSSM batte BotSort (-3.9% IDSW)

### Per Presentazione:
- Mostra entrambi i training curves
- Spiega precision vs recall trade-off
- Evidenzia importanza match_thresh=0.85 (parametro chiave)
- Conclusione: More epochs ‚â† better tracking (IDSW metric)

---

## üìù NOTE TECNICHE

### Batch Size Verificato
```
Log: "Batch size: 32"
Dataset: 21,332 images
Iterations: 666 per epoch
Total: 666 √ó 32 = 21,312 samples/epoch ‚úÖ
```

### Freeze Strategy Verificata
```
train_smooth.py line 62-71:
for name, param in model.named_parameters():
    if 'head' not in name:
        param.requires_grad = False  # Freeze
    else:
        param.requires_grad = True   # Train

Risultato:
  Frozen: 46,599,040 (86.1%)
  Trainable: 7,553,572 (13.9%) ‚úÖ
```

### Loss Components
```
total_loss = iou_loss + l1_loss + conf_loss + cls_loss

Epoch 1:  6.99 = 2.52 + 0.00 + 3.19 + 1.29
Epoch 30: 5.01 = 2.02 + 0.00 + 2.26 + 0.74

Reduction: -28.3% total, -42.6% class loss
```

---

**Data analisi**: 5 dicembre 2025  
**Analista**: Training optimization review  
**Status**: ‚úÖ Analisi completata, modifiche config V2 applicate, raccomandazione: usa epoch 10
